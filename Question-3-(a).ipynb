{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns in the dataframe\n",
      "['smart_1_normalized']\n",
      "All centroids\n",
      "[[4.2730568163083795], [6.726588196069323], [5.816484933988873], [4.722193215367085], [11.588285637607676]]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import os\n",
    "os.environ['PATH'].split(';')\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"midterm\").getOrCreate()\n",
    "\n",
    "df = spark.read .format(\"csv\").option(\"header\", \"true\").load(\"drive_stats_2019_Q1/*.csv\")\n",
    "df_temp = df.select(\"smart_1_normalized\")\n",
    "\n",
    "print(\"All columns in the dataframe\")\n",
    "print(df_temp.columns)\n",
    "feat_cols = [ 'smart_1_normalized']\n",
    "\n",
    "for column in feat_cols:\n",
    "    df_temp = df_temp.withColumn(column,df_temp[column].cast(IntegerType()))\n",
    "\n",
    "\n",
    "# Run the K-means on all the datapoints of the dataframe on column smart attribute smart_normalized_1.\n",
    "# For each point:\n",
    "# Predict the cluster they belong to. {0,1,2 ....}\n",
    "# Calculate the distance between the point and the centroid of that cluster.\n",
    "# Based on a given threshold, flag outliers.\n",
    "\n",
    "df_temp.dropna()\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "final_data = vec_assembler.setHandleInvalid(\"skip\").transform(df_temp)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(final_data)\n",
    "cluster_final_data = scalerModel.transform(final_data)\n",
    "\n",
    "# K>5 did show improvement in clustering\n",
    "\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=5)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)\n",
    "\n",
    "print(\"All centroids\")\n",
    "centers = model_k2.clusterCenters()\n",
    "centers = [center.tolist() for center in centers]\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers for clusters\n",
      "+------------------+----------+---------+----+\n",
      "|smart_1_normalized|prediction| distance|rank|\n",
      "+------------------+----------+---------+----+\n",
      "|               153|         1|2.1384923|   1|\n",
      "|               149|         1|1.9067256|   2|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "|               109|         1|0.4109425|   3|\n",
      "+------------------+----------+---------+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df_preds2 = model_k2.transform(cluster_final_data).orderBy(\"prediction\")\n",
    "\n",
    "distance_udf = F.udf(lambda x,y: float(distance.euclidean(x, centers[y])), FloatType())\n",
    "df_preds2 = df_preds2.withColumn('distance', distance_udf(F.col('scaledFeatures'),F.col('prediction')))\n",
    "df_preds2 = df_preds2.orderBy(\"prediction\").orderBy(F.desc(\"distance\"))\n",
    "# df_preds2.show()\n",
    "# q = df_preds2.approxQuantile(\"distance\", [0.5], 1)\n",
    "# q\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "window = Window.partitionBy(df_preds2['prediction']).orderBy(df_preds2['distance'].desc())\n",
    "\n",
    "threshold = 10\n",
    "print(\"Outliers for clusters\")\n",
    "df_preds2.select('smart_1_normalized','prediction','distance', rank().over(window).alias('rank')) .filter(F.col('rank') <= threshold) .show(50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
